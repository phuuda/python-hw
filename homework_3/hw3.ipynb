{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1 - WikiParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, re\n",
    "sys.path.append('/users/Sofia/desktop/python-hw/homework_2/pattern-2.6') # specific for my machine\n",
    "\n",
    "from pattern.web import Wikipedia\n",
    "\n",
    "class WikiParser:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def __get_text(self, search_term):\n",
    "        \n",
    "        article = Wikipedia().search(search_term)\n",
    "        if article:\n",
    "            s = article.string        \n",
    "            if s:\n",
    "                while '\\n' in s:\n",
    "                    s = s.replace('\\n', ' ')\n",
    "                while '  ' in s:\n",
    "                    s = s.replace('  ', ' ')\n",
    "\n",
    "                #s = re.sub(r'[^\\w\\s]','',s) # strips punctuation\n",
    "                # оставить знаки пунктуации, которые разделяют предложения\n",
    "                s = re.sub(r'[^\\w\\s^\\.^!^?]','',s)\n",
    "                s = s.lower()\n",
    "                return s\n",
    "    \n",
    "            else:\n",
    "                return None\n",
    "\n",
    "        else:\n",
    "            return None\n",
    "    \n",
    "    def get_articles(self, search_term):\n",
    "        text_list = []\n",
    "        \n",
    "        article = Wikipedia().search(search_term)\n",
    "        links = article.links\n",
    "        \n",
    "        s = self.__get_text(search_term)\n",
    "        text_list.append(s)\n",
    "        \n",
    "        for link in links:\n",
    "            text_list.append(self.__get_text(link))\n",
    "            print(link)\n",
    "\n",
    "        return text_list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Task 2 - TextStatistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import ngrams\n",
    "from collections import Counter\n",
    "import operator\n",
    "import math\n",
    "\n",
    "class TextStatistics:\n",
    "    def __init__(self, articles):        \n",
    "        self.articles = articles\n",
    "        pass\n",
    "\n",
    "    def get_top_3grams(self, n, use_idf=True):\n",
    "        \n",
    "        all_three_grams = []\n",
    "        \n",
    "        for article in self.articles:\n",
    "            #words = article.split()      \n",
    "            words = list(article)        # split by symbol\n",
    "             \n",
    "            #for word in words:          keep digits\n",
    "            #    if word.isdigit():\n",
    "            #        words.remove(word)\n",
    "                    \n",
    "            while '' in words:\n",
    "                words.remove('')\n",
    "\n",
    "            three_grams = ngrams(words, 3)\n",
    "            for gram in three_grams:\n",
    "                all_three_grams.append(gram)\n",
    "\n",
    "# use_idf, при выставлении которого частоты умножаются на коэффициент IDF и соотвественно сортируются.\n",
    "# IDF = log (<общее кол-во предложений в корпусе> / <кол-во предложений, в которых встретилась данная триграмма>)\n",
    "\n",
    "        if use_idf:\n",
    "\n",
    "            # get num of sentences (punctuations)\n",
    "            whole_text = ' '.join(self.articles)\n",
    "            sents = re.split(r' *[\\.\\?!][\\'\"\\)\\]]* *', whole_text)\n",
    "            \n",
    "            freq_list = {}\n",
    "            \n",
    "            for g in all_three_grams:\n",
    "                count_occurance = 0\n",
    "                for s in sents:\n",
    "                    s = str(s)\n",
    "                    if str(g) in s:\n",
    "                        count_occurance += 1\n",
    "                \n",
    "                if count_occurance > 0:\n",
    "                    idf = math.log(len(sents) / count_occurance)\n",
    "                else:\n",
    "                    idf = 0\n",
    "                    \n",
    "                freq_list[g] = idf\n",
    "                \n",
    "            freq_list = sorted(freq_list.items(), key=operator.itemgetter(1))\n",
    "            freq_list = freq_list[0:n-1]\n",
    "            \n",
    "        else:\n",
    "        \n",
    "            freq_list = Counter(tuple(i) for i in all_three_grams)\n",
    "            freq_list = freq_list.most_common(n)\n",
    "\n",
    "        list_of_3grams_in_descending_order_by_freq = []\n",
    "        list_of_their_corresponding_freq = []\n",
    "        \n",
    "        for i in freq_list:\n",
    "            list_of_3grams_in_descending_order_by_freq.append(i[0])\n",
    "            list_of_their_corresponding_freq.append(i[1])\n",
    "            \n",
    "        return (list_of_3grams_in_descending_order_by_freq, list_of_their_corresponding_freq)\n",
    "    \n",
    "    def get_top_words(self, n, use_idf=True):\n",
    "        \n",
    "        all_words, bad_list = [], ['a', 'an', 'the', 'aboard', 'about', 'above', 'across', 'between',\n",
    "                                   'afore', 'after', 'against', 'along', 'amid', 'amidst', 'among',\n",
    "                                   'amongst', 'around', 'as', 'aside', 'aslant', 'astride', 'at',\n",
    "                                   'athwart', 'atop', 'before', 'behind', 'below', 'beneath', 'beside',\n",
    "                                   'besides', 'between', 'betwixt', 'beyond', 'by', 'circa', 'despite',\n",
    "                                   'down', 'except', 'for', 'from', 'in', 'inside', 'into', 'like', 'near',\n",
    "                                   'neath', 'next', 'of', 'off', 'on', 'opposite', 'out', 'outside', 'over',\n",
    "                                   'per', 'through', 'till'', toward', 'towards', 'under', 'underneath',\n",
    "                                   'unlike', 'until', 'up', 'with', 'without']\n",
    "        \n",
    "        for article in self.articles:\n",
    "                      \n",
    "            words = article.split()\n",
    "            for word in words:\n",
    "                k = 0\n",
    "                for b in bad_list:\n",
    "                    if word == b:\n",
    "                        k += 1\n",
    "                        \n",
    "                if k == 0: \n",
    "                    if not word.isdigit(): \n",
    "                            if word != '':\n",
    "                                all_words.append(word)\n",
    "        \n",
    "        if use_idf: # added idf parameter     \n",
    "            \n",
    "            word_list = list(set(all_words))\n",
    "            word_count = {}\n",
    "            \n",
    "            for i in word_list:\n",
    "                word_occurances = 0\n",
    "                word = i[0]\n",
    "                for a in self.articles:\n",
    "                    if word in a:\n",
    "                        word_occurances += 1\n",
    "                \n",
    "                if word_occurances > 0:\n",
    "                    idf = math.log(len(self.articles) / word_occurances)\n",
    "                    \n",
    "                else:\n",
    "                    idf = 0\n",
    "                    \n",
    "                word_count[word] = idf\n",
    "                \n",
    "            # sort by most common, get top n\n",
    "            word_count = sorted(word_count.items(), key=operator.itemgetter(1))\n",
    "            word_count = word_count[0:n-1]\n",
    "            \n",
    "                \n",
    "        else:\n",
    "            word_count = Counter(word for word in all_words)\n",
    "            word_count = word_count.most_common(n)\n",
    "\n",
    "        list_of_words_in_descending_order_by_freq = []\n",
    "        list_of_their_corresponding_freq = []\n",
    "        \n",
    "        for i in word_count:\n",
    "            list_of_words_in_descending_order_by_freq.append(i[0])\n",
    "            list_of_their_corresponding_freq.append(i[1])\n",
    "        \n",
    "        return (list_of_words_in_descending_order_by_freq, list_of_their_corresponding_freq)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3 - Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tabulate import tabulate\n",
    "\n",
    "#from task_1 import Wikiparser\n",
    "#from task_2 import TextStatistics\n",
    "\n",
    "class Experiment:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def __phrase_print(self, term_list, freq_list):\n",
    "        \n",
    "        data = []\n",
    "        for a, b in zip(term_list, freq_list):\n",
    "            add = [' '.join(a), str(b)]\n",
    "            data.append(add)\n",
    "        print tabulate(data)\n",
    "        \n",
    "    def __word_print(self, term_list, freq_list):\n",
    "        \n",
    "        data = []\n",
    "        for a, b in zip(term_list, freq_list):\n",
    "            add = [''.join(a), str(b)]\n",
    "            data.append(add)\n",
    "        print tabulate(data)\n",
    "    \n",
    "    def show_results(self):\n",
    "        \n",
    "        a = WikiParser()\n",
    "        nlp_corpus = a.get_articles('Natural_language_processing')\n",
    "        nlp_text = [nlp_corpus[0]]\n",
    "\n",
    "        b = TextStatistics(nlp_text)\n",
    "\n",
    "        nlp_three_grams = b.get_top_3grams(5)\n",
    "        print('top 5 NLP page 3-grams:\\n')\n",
    "        self.__phrase_print(nlp_three_grams[0], nlp_three_grams[1])\n",
    "\n",
    "        nlp_top_words = b.get_top_words(5)\n",
    "        print('\\ntop 5 NLP page words:\\n')\n",
    "        self.__word_print(nlp_top_words[0], nlp_top_words[1])\n",
    "\n",
    "        c = TextStatistics(nlp_corpus)    \n",
    "\n",
    "        three_grams = c.get_top_3grams(20)\n",
    "        print('\\ntop 20 NLP Corpus 3-grams:\\n')\n",
    "        self.__phrase_print(three_grams[0], three_grams[1])\n",
    "\n",
    "        top_words = c.get_top_words(20)\n",
    "        print('\\ntop 20 NLP Corpus words:\\n')\n",
    "        self.__word_print(top_words[0], top_words[1])\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Выполнить парсинг статей википедии для \"Natural language processing\"\n",
    "#### По полученному корпусу текстов посчитать топ-20 3-грамм и топ-20 слов с use_idf=True."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#e = Experiment()\n",
    "#texts = e.show_results()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
